{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef359dca-3cbc-4051-b38b-1b8d489decfd",
   "metadata": {},
   "source": [
    "# This script is to download CMIP6 data using intake-esm library\n",
    "\n",
    "Read more from link\n",
    "\n",
    "https://intake-esm.readthedocs.io/en/stable/tutorials/loading-cmip6-data.html\n",
    "\n",
    "How to use intake to download GCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63679b9-8aa5-40ec-af02-1d86452529ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862884b6-92db-4a09-b358-a9f4fb2afae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/doan/anaconda3/envs/intake_esm/lib/python3.12/site-packages/fastprogress/fastprogress.py:107: UserWarning: Couldn't import ipywidgets properly, progress bar will use console behavior\n",
      "  warn(\"Couldn't import ipywidgets properly, progress bar will use console behavior\")\n"
     ]
    }
   ],
   "source": [
    "# Information of all CMIP6 files that one can download from intake esm data store \n",
    "url = \"https://raw.githubusercontent.com/NCAR/intake-esm-datastore/master/catalogs/pangeo-cmip6.json\"\n",
    "# open the catalog\n",
    "dataframe = intake.open_esm_datastore(url)\n",
    "#dataframe.df.columns\n",
    "#df = dataframe.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e798a33-6051-4272-9740-1bd26304c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is to sort files with name include\n",
    "    r1: Realization (initial condition run)\n",
    "    i1: Initialization method\n",
    "    p1: Physical parameters\n",
    "    f1: External forcings\n",
    "'''\n",
    "\n",
    "import re\n",
    "def natural_sort(l): \n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93778d6c-7b0a-4806-b84c-3b3b39c4fad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tas historical MRI-ESM2-0 11\n",
      "r1i1p1f1\n",
      "Download\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "write to  Download/MRI-ESM2-0/historical/tas_CMIP.MRI.MRI-ESM2-0.historical.Amon.gn_r1i1p1f1.nc\n",
      "ta historical MRI-ESM2-0 11\n",
      "r1i1p1f1\n",
      "Download\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "write to  Download/MRI-ESM2-0/historical/ta_CMIP.MRI.MRI-ESM2-0.historical.Amon.gn_r1i1p1f1.nc\n",
      "ua historical MRI-ESM2-0 10\n",
      "r1i1p1f1\n",
      "Download\n",
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n",
      "write to  Download/MRI-ESM2-0/historical/ua_CMIP.MRI.MRI-ESM2-0.historical.Amon.gn_r1i1p1f1.nc\n"
     ]
    }
   ],
   "source": [
    "for sid in [ 'EC-Earth3', 'MIROC6',  'MRI-ESM2-0', 'ACCESS-CM2', 'IPSL-CM6A-LR', 'MPI-ESM1-2-HR'][:1]:\n",
    "    \n",
    "    for exp in ['historical','ssp585', 'ssp126', 'ssp370','ssp245'][:]:\n",
    "        \n",
    "            for var in ['tas','ta','ua','va','hur','zg','ts'][:]:\n",
    "                # seach all files with information given above\n",
    "                models = dataframe.search(experiment_id=exp,\n",
    "                                          table_id='Amon',\n",
    "                                          variable_id=var,\n",
    "                                          source_id = sid,\n",
    "                                          #institution_id=ins,\n",
    "                                          #member_id=mem\n",
    "                                          )  \n",
    "                # then one might get several files with the same conditions\n",
    "                # r1: Realization (initial condition run)\n",
    "                # i1: Initialization method\n",
    "                # p1: Physical parameters\n",
    "                # f1: External forcings\n",
    "\n",
    "                print(var, exp, sid,  len(models.df))\n",
    "\n",
    "                # if no files exist then print out error\n",
    "                if len(models.df) == 0: print('*** \\n Erorrrr \\n')\n",
    "\n",
    "                # sort the possible files\n",
    "                ml = natural_sort(models.df.member_id.values)\n",
    "\n",
    "                # get the first one only then seach again\n",
    "                mem = ml[0]\n",
    "                model_s = dataframe.search(experiment_id=exp,\n",
    "                                          table_id='Amon',\n",
    "                                          variable_id=var,\n",
    "                                          source_id = sid,\n",
    "                                          #institution_id=ins,\n",
    "                                          member_id=mem\n",
    "                                          )                  \n",
    "\n",
    "                # if no files exist then print out error\n",
    "                if len(model_s.df) == 0: print('*** \\n Erorrrr \\n')\n",
    "\n",
    "                print(mem)\n",
    "\n",
    "                if len(model_s.df) > 0:\n",
    "                    print('Download')\n",
    "                    \n",
    "                    if True:\n",
    "                        \n",
    "                        try:\n",
    "                        \n",
    "                            datasets = model_s.to_dataset_dict(zarr_kwargs={'consolidated': True, \"decode_times\": True, \"use_cftime\": True })\n",
    "                            #datasets = models.to_dataset_dict(xarray_open_kwargs={\"consolidated\": True, \"decode_times\": True, \"use_cftime\": True})\n",
    "                            \n",
    "                            for k, v in datasets.items():\n",
    "                                odir = 'Download/'+sid+'/'+exp+'/'\n",
    "                                if not os.path.exists(odir): os.makedirs(odir)\n",
    "                                ofile = odir + var + '_'+ k + '_'+mem+'.nc'\n",
    "                                print('write to ',ofile)\n",
    "                                v.to_netcdf(ofile)\n",
    "                        except:\n",
    "                            print('fail')\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb9e8f-8088-47f5-8cef-ace754bda174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6562b90-8c50-4e03-b56e-71e35431fb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
