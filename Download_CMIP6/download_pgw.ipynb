{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef359dca-3cbc-4051-b38b-1b8d489decfd",
   "metadata": {},
   "source": [
    "# This script is to download CMIP6 data using intake-esm library\n",
    "\n",
    "Read more from link\n",
    "\n",
    "https://intake-esm.readthedocs.io/en/stable/tutorials/loading-cmip6-data.html\n",
    "\n",
    "How to use intake to download GCM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a6493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63679b9-8aa5-40ec-af02-1d86452529ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59313/2245448372.py:10: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import chunk_util\n",
    "import intake\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "862884b6-92db-4a09-b358-a9f4fb2afae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information of all CMIP6 files that one can download from intake esm data store\n",
    "url = \"https://raw.githubusercontent.com/NCAR/intake-esm-datastore/master/catalogs/pangeo-cmip6.json\"\n",
    "url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e798a33-6051-4272-9740-1bd26304c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def natural_sort(l: Iterable[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Sort names like r1i1p1f1, r1i2p1f1 in a natural (numeric) order.\n",
    "    - r1: Realization (initial condition run),\n",
    "    - i1: Initialization method,\n",
    "    - p1: Physical parameters,\n",
    "    - f1: External forcings.\n",
    "\n",
    "    Numeric order means that r1i1p1f1 < r2i1p1f1 < r11i1p1f1.\n",
    "\n",
    "    :param l: list of names to be sorted\n",
    "    \"\"\"\n",
    "\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", key)]\n",
    "    return sorted(l, key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93778d6c-7b0a-4806-b84c-3b3b39c4fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(catalog_url: str, sid: str, exp: str, var: str):\n",
    "    \"\"\"\n",
    "    Download files from the CMIP6 data store\n",
    "\n",
    "    :param catalog_url: intake esm data store\n",
    "    :param sid: source_id\n",
    "    :param exp: experiment_id\n",
    "    :param var: variable_id\n",
    "    \"\"\"\n",
    "\n",
    "    catalog = intake.open_esm_datastore(catalog_url)\n",
    "    models = catalog.search(\n",
    "        experiment_id=exp,\n",
    "        table_id=\"Amon\",\n",
    "        variable_id=var,\n",
    "        source_id=sid,\n",
    "    )\n",
    "    # then one might get several files with the same conditions\n",
    "    # r1: Realization (initial condition run)\n",
    "    # i1: Initialization method\n",
    "    # p1: Physical parameters\n",
    "    # f1: External forcings\n",
    "\n",
    "    # if no files exist then print out error\n",
    "    if len(models.df) == 0:\n",
    "        print(\"*** No data found for\", var, exp, sid)\n",
    "        return False\n",
    "\n",
    "    member_ids = natural_sort(models.df.member_id.values)\n",
    "\n",
    "    # get the first one only then seach again\n",
    "    first_member_id = member_ids[0]\n",
    "    first_member = catalog.search(\n",
    "        experiment_id=exp,\n",
    "        table_id=\"Amon\",\n",
    "        variable_id=var,\n",
    "        source_id=sid,\n",
    "        member_id=first_member_id,\n",
    "    )\n",
    "\n",
    "    # if no files exist then print out error\n",
    "    if len(first_member.df) == 0:\n",
    "        print(\n",
    "            \"*** This is impossible, there must be data for\",\n",
    "            var,\n",
    "            exp,\n",
    "            sid,\n",
    "            member_ids[0],\n",
    "        )\n",
    "        return False\n",
    "\n",
    "    odir = Path(\"Download\") / sid / exp\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def output_file_name(key):\n",
    "        return odir / f\"{var}_{key}_{first_member_id}.nc\"\n",
    "\n",
    "    try:\n",
    "        # If all output files exist, skip\n",
    "        if all((output_file_name(key)).exists() for key in first_member.keys()):\n",
    "            return True\n",
    "\n",
    "        datasets: dict[str, xr.Dataset] = first_member.to_dataset_dict(\n",
    "            xarray_open_kwargs={\"consolidated\": True}\n",
    "        )\n",
    "\n",
    "        for key, ds in datasets.items():\n",
    "            ofile = output_file_name(key)\n",
    "            tmp_ofile = ofile.with_suffix(\".tmp.nc\")\n",
    "\n",
    "            # Compression\n",
    "            encoding = {\n",
    "                var_name: {\n",
    "                    \"zlib\": True,\n",
    "                    \"complevel\": 1,\n",
    "                    \"chunksizes\": chunk_util.chunk_shape_nD(\n",
    "                        data.shape, chunkSize=64 * 2**10\n",
    "                    ),\n",
    "                }\n",
    "                for var_name, data in ds.data_vars.items()\n",
    "            }\n",
    "\n",
    "            # Save to temporary file first, and then rename to output file to\n",
    "            # avoid regarding corrupted file due to sudden termination as\n",
    "            # complete file.\n",
    "            ds.to_netcdf(\n",
    "                tmp_ofile, format=\"NETCDF4_CLASSIC\", engine=\"netcdf4\", encoding=encoding\n",
    "            )\n",
    "            tmp_ofile.rename(ofile)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"*** Couldn't download\", var, exp, sid, e)\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddace358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> The keys in the returned dictionary of datasets are constructed as follows:\n",
      "\t'activity_id.institution_id.source_id.experiment_id.table_id.grid_label'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "download_files(url, \"EC-Earth3\", \"historical\", \"tas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecd7a481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(source_ids, experiments, variables):\n",
    "    status = []\n",
    "\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as executor:\n",
    "        futures = []\n",
    "        status = []\n",
    "        for sid in source_ids:\n",
    "            for exp in experiments:\n",
    "                for var in variables:\n",
    "                    future = executor.submit(download_files, url, sid, exp, var)\n",
    "                    futures.append(future)\n",
    "                    status.append(\n",
    "                        {\n",
    "                            \"source_id\": sid,\n",
    "                            \"experiment\": exp,\n",
    "                            \"variable\": var,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        for future, stat in tqdm(zip(futures, status), total=len(futures)):\n",
    "            try:\n",
    "                success = future.result()\n",
    "            except Exception as e:\n",
    "                success = False\n",
    "                print(\"*** Error:\", e)\n",
    "\n",
    "            stat[\"success\"] = success\n",
    "\n",
    "    return pd.DataFrame(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f33a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = [\n",
    "    \"EC-Earth3\",\n",
    "    \"MIROC6\",\n",
    "    \"MRI-ESM2-0\",\n",
    "    \"ACCESS-CM2\",\n",
    "    \"IPSL-CM6A-LR\",\n",
    "    \"MPI-ESM1-2-HR\",\n",
    "]\n",
    "experiments = [\"historical\", \"ssp585\"]\n",
    "variables = [\"tas\", \"ta\", \"ua\", \"va\", \"hur\", \"zg\", \"ts\"]\n",
    "\n",
    "download_status = download_data(source_ids, experiments, variables)\n",
    "print(f\"Successfully download {download_status['success'].sum()} files\")\n",
    "print(\"The following files couldn't be downloaded\")\n",
    "print(download_status.query(\"~success\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
